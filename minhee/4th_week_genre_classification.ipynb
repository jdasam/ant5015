{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade gdown\n",
    "# !gdown 1-4elQY1C-n23u3QqomnLiI9CN9iPrWC3\n",
    "# !tar -xzf genres.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path(\"genres\")\n",
    "assert data_dir.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav_fns = list(data_dir.rglob(\"*.wav\")) # rglob: recursive glob\n",
    "len(wav_fns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "wav_fn = wav_fns[0]\n",
    "y, sr = torchaudio.load(wav_fn)\n",
    "\n",
    "import IPython.display as ipd\n",
    "# print(y.shape, sr)\n",
    "# ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sr = 16000\n",
    "resampler = torchaudio.transforms.Resample(sr, new_sr)\n",
    "y_resampled = resampler(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [00:41<00:00, 19.50it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, data_dir, target_sr=16000, is_test=False):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.wav_fns = list(self.data_dir.rglob(\"*.wav\"))\n",
    "        # rglob itself is a generator -> make it as a list\n",
    "\n",
    "        self.origin_freq = 22050\n",
    "        self.target_sr = target_sr\n",
    "        self.resampler = torchaudio.transforms.Resample(self.origin_freq, self.target_sr)\n",
    "        \n",
    "        self.is_test = is_test\n",
    "\n",
    "        self.audio_dur = 2 # TODO: get this as argument\n",
    "\n",
    "        self.audio_label_pairs = self.load_audio_label_pairs()\n",
    "        self.class_names = self.make_class_vocab()\n",
    "        \n",
    "\n",
    "    def load_audio_label_pairs(self):\n",
    "        audio_label_pairs = []\n",
    "        selected_fns = self.wav_fns[:800] if not self.is_test else self.wav_fns[800:]\n",
    "        for wav_fn in tqdm(selected_fns):\n",
    "            y, sr = torchaudio.load(wav_fn)\n",
    "            assert sr == self.origin_freq, f\"Expected {self.origin_freq} but got {sr}\"\n",
    "            y = self.resampler(y)\n",
    "            audio_label_pairs.append((y, wav_fn.parent.name))\n",
    "            # parent.name: genres/blues/blues.00000.wav -> blues\n",
    "            # use paired audio and label, to prevent the mismatch\n",
    "        return audio_label_pairs\n",
    "\n",
    "    def make_class_vocab(self):\n",
    "        class_names = [label for _, label in self.audio_label_pairs]\n",
    "        class_names = sorted(list(set(class_names)))\n",
    "        return class_names\n",
    "\n",
    "    # dataset class에 꼭 필요한 두 가지: __len__, __getitem__\n",
    "    def __len__(self):\n",
    "        return len(self.audio_label_pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        audio, label = self.audio_label_pairs[idx]\n",
    "\n",
    "        audio_len = audio.shape[1]\n",
    "        max_end_point = audio_len - self.target_sr * self.audio_dur\n",
    "        start_point = random.randint(0, max_end_point - 1)\n",
    "        audio = audio[:, start_point:start_point + self.target_sr * self.audio_dur]\n",
    "\n",
    "        return audio, self.class_names.index(label)\n",
    "\n",
    "dataset = Dataset('genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blues', 'classical', 'country', 'hiphop', 'jazz', 'metal', 'reggae', 'rock']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(478912, 480000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether audio shape are all the same\n",
    "min(set([audio.shape[1] for audio, _ in dataset.audio_label_pairs])), 16000*30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.utils.data.DataLoader\n",
    "- 자동으로 dataset의 getitem을 호출해서 하나의 텐서로 묶어줌\n",
    "    - 텐서로 변환하는 함수: collate_fn\n",
    "    - 매 배치마다 어떤 idx로 dataset getitem을 호출할지 자동으로 결정\n",
    "    - shuffle=True면 자동으로 셔플링 된 샘플 호출\n",
    "    - 매 epoch마다 shuffle 순서 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 32000]), torch.Size([32]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "audios, labels = batch\n",
    "audios.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Model\n",
    "- Mel Spectrogram Converter\n",
    "- nn.Linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 80, 63])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MelDbConverter(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.mel_conv = torchaudio.transforms.MelSpectrogram(n_fft=1024,\n",
    "                                                         hop_length=512,\n",
    "                                                         f_min=20,\n",
    "                                                         f_max=4000,\n",
    "                                                         n_mels=80,\n",
    "                                                         sample_rate=16000)\n",
    "    self.db_conv = torchaudio.transforms.AmplitudeToDB()\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.db_conv(self.mel_conv(x))\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, dim=128, num_out = 10):\n",
    "        super().__init__() # init nn.Module\n",
    "\n",
    "        self.spec = MelDbConverter()\n",
    "        self.layers = nn.Sequential(\n",
    "           nn.Linear(5040, dim),\n",
    "           nn.ReLU(),\n",
    "           nn.Linear(dim, dim),\n",
    "           nn.ReLU(),\n",
    "          nn.Linear(dim, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        spec = self.spec(x)\n",
    "        flattened = spec.reshape(spec.shape[0], -1)\n",
    "        out = self.layers(flattened)\n",
    "        \n",
    "        return self.spec(x)\n",
    "    \n",
    "model = Model()\n",
    "model(audios).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spec Converter\n",
    "Audio -> spectrogram으로 옮기는 과정이\n",
    "1) model에 들어갈 수도 있고\n",
    "2) dataset 처리의 일부로 처리할 수도 있음\n",
    "- 이 경우, audio -> spectrogram으로 변환하는 데 필요한 n_fft, hop_size 등등의 내용까지 모두 동일하게 설정해야 모델이 올바르게 작동하게 됨\n",
    "- 예를 들어 f_max 설정이 원래와 달라졌음에도 불구하고 spectrogram의 shape은 동일해서, 돌아가긴 함에도 불구하고 이상한 결과가 나오는 상황이 발생할 수도.\n",
    "\n",
    "### 따라서\n",
    "- spec converter도 모델의 일부로 둬야 한다!!\n",
    "\n",
    "### model.state_dict(), model.parameters()\n",
    "- spec converter와 관련된 건 state_dict에는 포함\n",
    "    - model.state_dict().keys()를 찍어보면 spec converter와 관련된 것들이 있음\n",
    "- however! model.named_parameters()[0] 이렇게 학습되는 parameter의 이름들을 뽑아보면, spec converter에 관한 건 없음\n",
    "- 이렇게 모델에는 있지만 학습되는 건 아닌 부분은 'buffer'라고 부른다\n",
    "- 모델 내부에 buffer를 두면 모델을 gpu에 올릴 때 이 부분도 동일한 device로 가게 되어서 편리함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Number of Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight torch.Size([128, 5040]) 645120\n",
      "layers.0.bias torch.Size([128]) 128\n",
      "layers.2.weight torch.Size([128, 128]) 16384\n",
      "layers.2.bias torch.Size([128]) 128\n",
      "layers.4.weight torch.Size([10, 128]) 1280\n",
      "layers.4.bias torch.Size([10]) 10\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.numel())\n",
    "\n",
    "    \"\"\"\n",
    "    Result:\n",
    "    layers.0.weight torch.Size([128, 5040]) 645120\n",
    "    layers.0.bias torch.Size([128]) 128\n",
    "    layers.2.weight torch.Size([128, 128]) 16384\n",
    "    layers.2.bias torch.Size([128]) 128\n",
    "    layers.4.weight torch.Size([10, 128]) 1280\n",
    "    layers.4.bias torch.Size([10]) 10\n",
    "\n",
    "    중간에 있는 ReLU는 학습되지 않기 때문에 여기 없음\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/Validation Set\n",
    "- test 할 때마다 random하게 자르는 거 하면 안 된다 (train과 다르게)\n",
    "- testset을 고정 시켜놓아서, 결과가 달라지는 것이 온전히 모델에 의한 것이도록 해야 함. testset의 randomness에 영향을 받는 게 아닌."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:09<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "# For validation or test:\n",
    "# use larger batch size (it is okay since calculation cost is smaller than training)\n",
    "# Do not shuffle\n",
    "test_loader = DataLoader(Dataset('genres', is_test=True), batch_size=200, shuffle=False, drop_last=False)\n",
    "# Drop Last:\n",
    "# Training 할 때는 쓸 수도 있음\n",
    "# Test/Validation: Do not drop last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() # change dropout, batchnorm, etc. to evaluation mode\n",
    "model.to('cuda')\n",
    "with torch.inference_mode(): # do not need to calculate gradient\n",
    "    # torch.no_grad() is about not updating the gradient,\n",
    "    # but torch.inference_mode() is about not calculating the gradient so it is slightly faster\n",
    "\n",
    "    total_loss = []\n",
    "    for batch in test_loader:\n",
    "        audios, labels = batch\n",
    "        prob = model(audios.to('cuda'))\n",
    "        loss = -torch.log(prob[torch.arange(len(labels)), labels])\n",
    "        total_loss.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO(minigb): Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Particular Index of the tensor\n",
    "- torch.argmax\n",
    "- torch.argsort: sort한 결과에서의 idx 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 확인하기\n",
    "- model의 결과를 들어보면서 실험 하기\n",
    "    - loss가 높은 거 top5\n",
    "    - loss가 낮은 거 top5\n",
    "    - 이런 식으로 어떤 example을 잘 맞추고, 어떤 걸 잘 못 맞추는지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "- 현재 spectrogram shape: (32, 80, 63)\n",
    "    - 지금은 channel이 없는 상태\n",
    "    - conv layer가 sweep 하는 동안 input에서 같은 위치의 여러 channel이 모두 확인된다.\n",
    "- spec.unsqueeze(1): add empty dimension on 1st axis -> shape은 (32, 1, 80, 63). 채널 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze(1)로 in channel을 하나 만들었다.\n",
    "kernel = nn.Conv2d(in_channels=1, out_channels=2, kernel_size=3)\n",
    "# stride: 1, padding: 0 by default\n",
    "kernel.weight.shape\n",
    "\n",
    "# M) in_channel과 out_channel이 좀 헷갈렸는데\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "- duration 더 길게 해서 보면 더 높은 accuracy.\n",
    "- 전반적으로 더 높은 confidence를 가지게 됨\n",
    "- 그래서 loss가 낮은 test set을 확인해보면 그 loss는 이전보다 더 커짐\n",
    "- 전반적으로 overfitting 되는 것도 있고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
