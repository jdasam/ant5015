{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/ant5015/blob/main/MIR_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MUYF1dAwrNA1"
      },
      "source": [
        "# Assignment 4. Automatic Music Transcription\n",
        "- In this assignment, you have to implement AMT model using CNN and LSTM\n",
        "- You will use the pre-defined Dataset and small portion of MAESTRO Dataset\n",
        "\n",
        "- Problems:\n",
        "  - 1. Implement the AMT model using LSTM (10 pts)\n",
        "  - 2. Implement the AMT model using CNN + LSTM (10 pts)\n",
        "  - 3. Implement the Onsets and Frames model using CNN + LSTM (15 pts)\n",
        "  - 4. Transcription Test (15 pts)\n",
        "- Submission:\n",
        "  - Report (in pdf format)\n",
        "  - Code (in .ipynb format)\n",
        "  - Audio file for transcription test and output MIDI\n",
        "\n",
        "- The code of this assignment is based on the following repository:\n",
        "  - https://github.com/juhannam/gct634-2022/tree/main/hw3 (KAIST GCT634 by Prof. Juhan Nam. Assignment code was largely done by the TA Taegyun Kwon)\n",
        "  - https://github.com/jongwook/onsets-and-frames (PyTorch implementation of Onsets and Frames by Dr. Jongwook Kim)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V5qzuLafrNA2"
      },
      "source": [
        "## Prepare\n",
        "- Download dataset\n",
        "- Install requirements\n",
        "- Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5WFi264rNA3"
      },
      "outputs": [],
      "source": [
        "!gdown --id 1L99FSzloXGQsmc-OwsVKggvQNF9niN2M\n",
        "!unzip -q maestro_small.zip\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LGTby1MprNA3"
      },
      "source": [
        "### Install requirements, import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMO7JypBrNA3"
      },
      "outputs": [],
      "source": [
        "!apt install fluidsynth\n",
        "!pip install mido pretty_midi mir_eval soundfile pyFluidSynth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOhvqmVlrNA3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import soundfile\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "from typing import Dict, List\n",
        "from numpy import ndarray\n",
        "from numpy.random.mtrand import RandomState\n",
        "from pretty_midi.pretty_midi import PrettyMIDI\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "from torch import Tensor\n",
        "from torchaudio import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pretty_midi\n",
        "import fluidsynth\n",
        "\n",
        "from mido import Message, MidiFile, MidiTrack\n",
        "from mir_eval.transcription import precision_recall_f1_overlap as evaluate_notes\n",
        "from mir_eval.util import hz_to_midi, midi_to_hz\n",
        "\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "MIN_MIDI = 21\n",
        "MAX_MIDI = 108\n",
        "\n",
        "HOP_SIZE = 512\n",
        "N_MELS = 229\n",
        "N_FFT = 2048\n",
        "F_MIN = 30\n",
        "F_MAX = 8000"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "on7v7zC2rNA3"
      },
      "source": [
        "### Pre-defined code blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dcpcuU9rNA3"
      },
      "outputs": [],
      "source": [
        "def allocate_batch(batch:Dict[str,Tensor], device:torch.device) -> Dict[str,Tensor]:\n",
        "    for key in batch.keys():\n",
        "        if key != 'path':\n",
        "            batch[key] = batch[key].to(device)\n",
        "    return batch\n",
        "\n",
        "class MAESTRO_small(Dataset):\n",
        "    def __init__(self,\n",
        "                 path:str = 'data',\n",
        "                 groups:List[str] = None,\n",
        "                 sequence_length:int = SAMPLE_RATE * 5,\n",
        "                 hop_size:int = HOP_SIZE,\n",
        "                 seed:int = 42,\n",
        "                 random_sample:bool = True) -> None:\n",
        "        self.path:str = path\n",
        "        self.groups:list = groups if groups is not None else self.available_groups()\n",
        "        assert all(group in self.available_groups() for group in self.groups)\n",
        "\n",
        "        self.sample_length:int = ((sequence_length // hop_size) * hop_size) if sequence_length is not None else None\n",
        "        self.random:RandomState = np.random.RandomState(seed)\n",
        "        self.random_sample:bool = random_sample\n",
        "        self.hop_size:int = hop_size\n",
        "\n",
        "        self.data:List[Dict[str,Tensor]] = []\n",
        "\n",
        "        print(f'Loading {len(groups)} group(s) of', self.__class__.__name__, 'at', path)\n",
        "        for group in groups:\n",
        "            file_list:List[tuple] = self.get_file_path_list_of_group(group)\n",
        "            for input_files in tqdm(file_list, desc=f'Loading group {group}'):\n",
        "                self.data.append(self.load(*input_files))\n",
        "\n",
        "    @classmethod\n",
        "    def available_groups(cls) -> List[str]:\n",
        "        return ['train', 'validation', 'test', 'debug']\n",
        "\n",
        "    def get_file_path_list_of_group(self, group:str) -> List[tuple]:\n",
        "        metadata:List[dict] = json.load(open(os.path.join(self.path, 'data.json')))\n",
        "        subset_name:str = 'train' if group == 'debug' else group\n",
        "\n",
        "        files:List[tuple] = sorted([\n",
        "                (os.path.join(self.path, row['audio_filename'].replace('.wav', '.flac')),\n",
        "                 os.path.join(self.path, row['midi_filename']))\n",
        "                for row in metadata if row['split'] == subset_name\n",
        "            ])\n",
        "\n",
        "        if group == 'debug' or group == 'test': # use only 10 files for debug and test\n",
        "            files = files[:10]\n",
        "\n",
        "        files = [(audio if os.path.exists(audio) else audio.replace(\n",
        "                '.flac', '.wav'), midi) for audio, midi in files]\n",
        "\n",
        "        return files\n",
        "\n",
        "    def load(self, audio_path:str, midi_path:str) -> Dict[str,Tensor]:\n",
        "        \"\"\"Loads an audio track and the corresponding labels.\"\"\"\n",
        "        audio, sr = soundfile.read(audio_path, dtype='int16')\n",
        "        assert sr == SAMPLE_RATE\n",
        "\n",
        "        frames_per_sec:float = sr / self.hop_size\n",
        "\n",
        "        audio_tensor:Tensor = torch.ShortTensor(audio)\n",
        "        audio_length:int = len(audio_tensor)\n",
        "\n",
        "        midi:PrettyMIDI = pretty_midi.PrettyMIDI(midi_path)\n",
        "        midi_length_sec:float = midi.get_end_time()\n",
        "        frame_length:int = min(int(midi_length_sec * frames_per_sec), (audio_length // self.hop_size) + 1)\n",
        "\n",
        "        audio_tensor = audio_tensor[:frame_length * self.hop_size]\n",
        "\n",
        "        frame:ndarray = midi.get_piano_roll(fs=frames_per_sec)\n",
        "\n",
        "        onset = np.zeros_like(frame)\n",
        "        for inst in midi.instruments:\n",
        "            for note in inst.notes:\n",
        "                onset[note.pitch, int(note.start * frames_per_sec)] = 1\n",
        "\n",
        "        # to shape (time, pitch (88))\n",
        "        frame_tensor:Tensor = torch.from_numpy(frame[MIN_MIDI:MAX_MIDI + 1].T)\n",
        "        onset_tensor:Tensor = torch.from_numpy(onset[MIN_MIDI:MAX_MIDI + 1].T)\n",
        "        data = dict(path=audio_path, audio=audio_tensor, frame=frame_tensor, onset=onset_tensor)\n",
        "        return data\n",
        "\n",
        "    def __getitem__(self, index:int) -> Dict[str,Tensor]:\n",
        "        data:Dict[str,Tensor] = self.data[index]\n",
        "\n",
        "        audio:Tensor = data['audio']\n",
        "        frames:Tensor = (data['frame'] >= 1)\n",
        "        onsets:Tensor = (data['onset'] >= 1)\n",
        "\n",
        "        frame_len:int = frames.shape[0]\n",
        "\n",
        "        if self.sample_length is not None:\n",
        "            n_steps:int = self.sample_length // self.hop_size\n",
        "\n",
        "            step_begin:int = self.random.randint(frame_len - n_steps) if self.random_sample else 0\n",
        "            step_end:int = step_begin + n_steps\n",
        "\n",
        "            sample_begin:int = step_begin * self.hop_size\n",
        "            sample_end:int = sample_begin + self.sample_length\n",
        "\n",
        "            audio_seg:Tensor = audio[sample_begin:sample_end]\n",
        "            frame_seg:Tensor = frames[step_begin:step_end]\n",
        "            onset_seg:Tensor = onsets[step_begin:step_end]\n",
        "\n",
        "            result = dict(path=data['path'])\n",
        "            result['audio'] = audio_seg.float().div_(32768.0)\n",
        "            result['frame'] = frame_seg.float()\n",
        "            result['onset'] = onset_seg.float()\n",
        "        else:\n",
        "            result = dict(path=data['path'])\n",
        "            result['audio'] = audio.float().div_(32768.0)\n",
        "            result['frame'] = frames.float()\n",
        "            result['onset'] = onsets.float()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "def evaluate(model, batch, device, save=False, save_path=None):\n",
        "    metrics = defaultdict(list)\n",
        "    batch = allocate_batch(batch, device)\n",
        "\n",
        "    frame_logit, onset_logit = model(batch['audio'])\n",
        "\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    frame_loss = criterion(frame_logit, batch['frame'])\n",
        "    onset_loss = criterion(onset_logit, batch['onset'])\n",
        "    metrics['metric/loss/frame_loss'].append(frame_loss.cpu().numpy())\n",
        "    metrics['metric/loss/onset_loss'].append(onset_loss.cpu().numpy())\n",
        "\n",
        "    for batch_idx in range(batch['audio'].shape[0]):\n",
        "        frame_pred = torch.sigmoid(frame_logit[batch_idx])\n",
        "        onset_pred = torch.sigmoid(onset_logit[batch_idx])\n",
        "\n",
        "        pr, re, f1 = framewise_eval(frame_pred, batch['frame'][batch_idx])\n",
        "        metrics['metric/frame/frame_precision'].append(pr)\n",
        "        metrics['metric/frame/frame_recall'].append(re)\n",
        "        metrics['metric/frame/frame_f1'].append(f1)\n",
        "\n",
        "        pr, re, f1 = framewise_eval(onset_pred, batch['onset'][batch_idx])\n",
        "        metrics['metric/frame/onset_precision'].append(pr)\n",
        "        metrics['metric/frame/onset_recall'].append(re)\n",
        "        metrics['metric/frame/onset_f1'].append(f1)\n",
        "\n",
        "        p_est, i_est = extract_notes(onset_pred, frame_pred)\n",
        "        p_ref, i_ref = extract_notes(\n",
        "            batch['onset'][batch_idx], batch['frame'][batch_idx])\n",
        "\n",
        "        scaling = HOP_SIZE / SAMPLE_RATE\n",
        "\n",
        "        i_ref = (i_ref * scaling).reshape(-1, 2)\n",
        "        p_ref = np.array([midi_to_hz(MIN_MIDI + pitch) for pitch in p_ref])\n",
        "        i_est = (i_est * scaling).reshape(-1, 2)\n",
        "        p_est = np.array([midi_to_hz(MIN_MIDI + pitch) for pitch in p_est])\n",
        "\n",
        "        p, r, f, o = evaluate_notes(\n",
        "            i_ref, p_ref, i_est, p_est, offset_ratio=None)\n",
        "        metrics['metric/note/precision'].append(p)\n",
        "        metrics['metric/note/recall'].append(r)\n",
        "        metrics['metric/note/f1'].append(f)\n",
        "        metrics['metric/note/overlap'].append(o)\n",
        "\n",
        "        p, r, f, o = evaluate_notes(i_ref, p_ref, i_est, p_est)\n",
        "        metrics['metric/note-with-offsets/precision'].append(p)\n",
        "        metrics['metric/note-with-offsets/recall'].append(r)\n",
        "        metrics['metric/note-with-offsets/f1'].append(f)\n",
        "        metrics['metric/note-with-offsets/overlap'].append(o)\n",
        "\n",
        "        if save:\n",
        "            stem = Path(batch[\"path\"][batch_idx]).stem\n",
        "            if len(p_est) == 0:\n",
        "                print(f'No onset detected. Skip: {stem}')\n",
        "            midi_filename = Path(save_path) / (stem + '.midi')\n",
        "            save_midi(midi_filename, p_est, i_est, [64] * len(p_est))\n",
        "\n",
        "            wav_filename = Path(save_path) / (stem + '.wav')\n",
        "            midi_file = pretty_midi.PrettyMIDI(str(midi_filename))\n",
        "            synth_audio = midi_file.fluidsynth(fs=16000)\n",
        "            soundfile.write(wav_filename, synth_audio, 16000)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def extract_notes(onsets, frames, onset_threshold=0.5, frame_threshold=0.5):\n",
        "    \"\"\"Finds the note timings based on the onsets and frames information.\n",
        "\n",
        "    Args:\n",
        "        onsets: torch.FloatTensor of shape (frames, bins)\n",
        "        frames: torch.FloatTensor of shape (frames, bins)\n",
        "        onset_threshold: float\n",
        "        frame_threshold: float\n",
        "\n",
        "    Returns:\n",
        "        pitches: np.ndarray of bin_indices\n",
        "        intervals: np.ndarray of rows containing (onset_index, offset_index)\n",
        "    \"\"\"\n",
        "    onsets = (onsets > onset_threshold).type(torch.int).cpu()\n",
        "    frames = (frames > frame_threshold).type(torch.int).cpu()\n",
        "    onset_diff = torch.cat([onsets[:1, :], onsets[1:, :] - onsets[:-1, :]],\n",
        "                           dim=0) == 1\n",
        "\n",
        "    pitches = []\n",
        "    intervals = []\n",
        "\n",
        "    for nonzero in onset_diff.nonzero():\n",
        "        frame = nonzero[0].item()\n",
        "        pitch = nonzero[1].item()\n",
        "\n",
        "        onset = frame\n",
        "        offset = frame\n",
        "\n",
        "        while onsets[offset, pitch].item() or frames[offset, pitch].item():\n",
        "            offset += 1\n",
        "            if offset == onsets.shape[0]:\n",
        "                break\n",
        "            if (offset != onset) and onsets[offset, pitch].item():\n",
        "                break\n",
        "\n",
        "        if offset > onset:\n",
        "            pitches.append(pitch)\n",
        "            intervals.append([onset, offset])\n",
        "\n",
        "    return np.array(pitches), np.array(intervals)\n",
        "\n",
        "\n",
        "def framewise_eval(pred, label, threshold=0.5):\n",
        "    '''Evaluates frame-wise (point-wise) evaluation.\n",
        "\n",
        "    Args:\n",
        "        pred: torch.Tensor of shape (frame, pitch)\n",
        "        label: torch.Tensor of shape (frame, pitch)\n",
        "    '''\n",
        "\n",
        "    tp = torch.sum((pred >= threshold) * (label == 1)).cpu().numpy()\n",
        "    fn = torch.sum((pred < threshold) * (label == 1)).cpu().numpy()\n",
        "    fp = torch.sum((pred >= threshold) * (label != 1)).cpu().numpy()\n",
        "\n",
        "    pr = tp / float(tp + fp) if (tp + fp) > 0 else 0\n",
        "    re = tp / float(tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * pr * re / float(pr + re) if (pr + re) > 0 else 0\n",
        "\n",
        "    return pr, re, f1\n",
        "\n",
        "\n",
        "def save_midi(path, pitches, intervals, velocities):\n",
        "    \"\"\"Saves extracted notes as a MIDI file.\n",
        "\n",
        "    Args:\n",
        "        path: the path to save the MIDI file\n",
        "        pitches: np.ndarray of bin_indices\n",
        "        intervals: list of tuple (onset_index, offset_index)\n",
        "        velocities: list of velocity values\n",
        "    \"\"\"\n",
        "    file = MidiFile()\n",
        "    track = MidiTrack()\n",
        "    file.tracks.append(track)\n",
        "    ticks_per_second = file.ticks_per_beat * 2.0\n",
        "\n",
        "    events = []\n",
        "    for i in range(len(pitches)):\n",
        "        events.append(\n",
        "            dict(type='on',\n",
        "                 pitch=pitches[i],\n",
        "                 time=intervals[i][0],\n",
        "                 velocity=velocities[i]))\n",
        "        events.append(\n",
        "            dict(type='off',\n",
        "                 pitch=pitches[i],\n",
        "                 time=intervals[i][1],\n",
        "                 velocity=velocities[i]))\n",
        "    events.sort(key=lambda row: row['time'])\n",
        "\n",
        "    last_tick = 0\n",
        "    for event in events:\n",
        "        current_tick = int(event['time'] * ticks_per_second)\n",
        "        velocity = int(event['velocity'] * 127)\n",
        "        if velocity > 127:\n",
        "            velocity = 127\n",
        "        pitch = int(round(hz_to_midi(event['pitch'])))\n",
        "        track.append(\n",
        "            Message('note_' + event['type'],\n",
        "                    note=pitch,\n",
        "                    velocity=velocity,\n",
        "                    time=current_tick - last_tick))\n",
        "        last_tick = current_tick\n",
        "\n",
        "    file.save(path)\n",
        "\n",
        "def cycle(iterable):\n",
        "    while True:\n",
        "        for item in iterable:\n",
        "            yield item\n",
        "\n",
        "\n",
        "def train(model_type,\n",
        "          logdir,\n",
        "          dataset,\n",
        "          valid_dataset,\n",
        "          batch_size,\n",
        "          iterations,\n",
        "          validation_interval,\n",
        "          sequence_length,\n",
        "          learning_rate,\n",
        "          weight_decay,\n",
        "          cnn_unit,\n",
        "          fc_unit,\n",
        "          debug=False,\n",
        "          save_midi=False):\n",
        "    if logdir is None:\n",
        "        logdir = Path('runs') / ('exp_' +\n",
        "                                 datetime.now().strftime('%y%m%d-%H%M%S'))\n",
        "    Path(logdir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if sequence_length % HOP_SIZE != 0:\n",
        "        adj_length = sequence_length // HOP_SIZE * HOP_SIZE\n",
        "        print(\n",
        "            f'sequence_length {sequence_length} is not a multiple of {HOP_SIZE}.'\n",
        "        )\n",
        "        print(f'Adjusted to: {adj_length}')\n",
        "        sequence_length = adj_length\n",
        "    loader = DataLoader(dataset, batch_size, shuffle=True)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if model_type == 'baseline':\n",
        "        model = Transcriber(cnn_unit=cnn_unit, fc_unit=fc_unit)\n",
        "    elif model_type == 'rnn':\n",
        "        model = Transcriber_RNN(cnn_unit=cnn_unit, fc_unit=fc_unit)\n",
        "    elif model_type == 'crnn':\n",
        "        model = Transcriber_CRNN(cnn_unit=cnn_unit, fc_unit=fc_unit)\n",
        "    elif model_type == 'ONF':\n",
        "        model = Transcriber_ONF(cnn_unit=cnn_unit, fc_unit=fc_unit)\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 learning_rate,\n",
        "                                 weight_decay=weight_decay)\n",
        "    scheduler = StepLR(optimizer, step_size=1000, gamma=0.98)\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    loop = tqdm(range(1, iterations + 1))\n",
        "\n",
        "    for step, batch in zip(loop, cycle(loader)):\n",
        "        optimizer.zero_grad()\n",
        "        batch = allocate_batch(batch, device)\n",
        "\n",
        "        frame_logit, onset_logit = model(batch['audio'])\n",
        "        frame_loss = criterion(frame_logit, batch['frame'])\n",
        "        onset_loss = criterion(onset_logit, batch['onset'])\n",
        "        loss = onset_loss + frame_loss\n",
        "\n",
        "        loss.mean().backward()\n",
        "\n",
        "        for parameter in model.parameters():\n",
        "            clip_grad_norm_([parameter], 3.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        loop.set_postfix_str(\"loss: {:.3e}\".format(loss.mean()))\n",
        "\n",
        "        if step % validation_interval == 0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                loader = DataLoader(valid_dataset,\n",
        "                                    batch_size=batch_size,\n",
        "                                    shuffle=False)\n",
        "                metrics = defaultdict(list)\n",
        "                for batch in loader:\n",
        "                    batch_results = evaluate(model, batch, device)\n",
        "\n",
        "                    for key, value in batch_results.items():\n",
        "                        metrics[key].extend(value)\n",
        "            print('')\n",
        "            for key, value in metrics.items():\n",
        "                if key[-2:] == 'f1' or 'loss' in key:\n",
        "                    print(f'{key:27} : {np.mean(value):.4f}')\n",
        "            model.train()\n",
        "\n",
        "    state_dict = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'step': step,\n",
        "        'cnn_unit': cnn_unit,\n",
        "        'fc_unit': fc_unit\n",
        "    }\n",
        "    torch.save(state_dict, Path(logdir) / f'model-{step}.pt')\n",
        "    del dataset, valid_dataset\n",
        "\n",
        "    test_dataset = MAESTRO_small(groups=['test'],\n",
        "                                 hop_size=HOP_SIZE,\n",
        "                                 random_sample=False)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "        metrics = defaultdict(list)\n",
        "        for batch in loader:\n",
        "            batch_results = evaluate(model,\n",
        "                                     batch,\n",
        "                                     device,\n",
        "                                     save=save_midi,\n",
        "                                     save_path=logdir)\n",
        "            for key, value in batch_results.items():\n",
        "                metrics[key].extend(value)\n",
        "    print('')\n",
        "    for key, value in metrics.items():\n",
        "        if key[-2:] == 'f1' or 'loss' in key:\n",
        "            print(f'{key} : {np.mean(value)}')\n",
        "\n",
        "    with open(Path(logdir) / 'results.txt', 'w') as f:\n",
        "        for key, values in metrics.items():\n",
        "            _, category, name = key.split('/')\n",
        "            metric_string = f'{category:>32} {name:26}: '\n",
        "            metric_string += f'{np.mean(values):.3f} +- {np.std(values):.3f}'\n",
        "            print(metric_string)\n",
        "            f.write(metric_string + '\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp6w4jMfrNA5"
      },
      "outputs": [],
      "source": [
        "# Load validation dataset as Example dataset\n",
        "validation_dataset = MAESTRO_small(path='data',\n",
        "                                   groups=['validation'],\n",
        "                                   sequence_length=102400,\n",
        "                                   hop_size=HOP_SIZE,\n",
        "                                   random_sample=False)\n",
        "example_loader = DataLoader(validation_dataset, batch_size=2, shuffle=False)\n",
        "batch = next(iter(example_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIyty08jrNA5"
      },
      "outputs": [],
      "source": [
        "data = validation_dataset[0]\n",
        "print(f'data: {data}')\n",
        "print(f'audio_shape: {data[\"audio\"].shape}')\n",
        "print(f'frame_roll_shape: {data[\"frame\"].shape}')\n",
        "print(f'onset_roll_shape: {data[\"onset\"].shape}')\n",
        "\n",
        "print(f'HOP_SIZE({HOP_SIZE}) x piano_roll length({data[\"frame\"].shape[0]}): {HOP_SIZE*data[\"frame\"].shape[0]}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MloYMbMqrNA5"
      },
      "outputs": [],
      "source": [
        "idx = 1 # You can test different idx\n",
        "\n",
        "data = validation_dataset[idx]\n",
        "ipd.display(ipd.Audio(data['audio'].numpy()[:400*HOP_SIZE], rate=SAMPLE_RATE))\n",
        "\n",
        "plt.figure(figsize=(10,15))\n",
        "plt.subplot(311)\n",
        "plt.plot(data['audio'].numpy()[:400*HOP_SIZE])\n",
        "plt.autoscale(enable=True, axis='x', tight=True)\n",
        "plt.subplot(312)\n",
        "plt.imshow(data['frame'].numpy()[:400].T, aspect='auto', origin='lower', interpolation='nearest')\n",
        "plt.subplot(313)\n",
        "plt.imshow(data['onset'].numpy()[:400].T, aspect='auto', origin='lower', interpolation='nearest')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tCRrvRbRrNA5"
      },
      "source": [
        "# 0. Define Baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSCkVey1rNA5"
      },
      "outputs": [],
      "source": [
        "# Define Training hyperparameters\n",
        "\n",
        "sequence_length = 102400\n",
        "lr = 6e-4\n",
        "batch_size = 16\n",
        "iterations = 5000\n",
        "validation_interval = 500\n",
        "weight_decay = 0\n",
        "cnn_unit = 48\n",
        "fc_unit = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GTTUNWFuqrE"
      },
      "outputs": [],
      "source": [
        "dataset = MAESTRO_small(path='data',\n",
        "                        groups=['train'],\n",
        "                        sequence_length=sequence_length,\n",
        "                        hop_size=HOP_SIZE,\n",
        "                        random_sample=True)\n",
        "\n",
        "# validation_dataset was declared in the cell above\n",
        "# validation_dataset = MAESTRO_small(path='data',\n",
        "#                                    groups=['validation'],\n",
        "#                                    sequence_length=sequence_length,\n",
        "#                                    hop_size=HOP_SIZE,\n",
        "#                                    random_sample=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GInTS2AzrNA5"
      },
      "outputs": [],
      "source": [
        "class LogMelSpectrogram(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.melspectrogram = transforms.MelSpectrogram(\n",
        "            sample_rate=SAMPLE_RATE,\n",
        "            n_fft=N_FFT,\n",
        "            hop_length=HOP_SIZE,\n",
        "            f_min=F_MIN,\n",
        "            f_max=F_MAX,\n",
        "            n_mels=N_MELS,\n",
        "            normalized=False)\n",
        "\n",
        "    def forward(self, audio):\n",
        "        # Alignment correction to match with piano roll.\n",
        "        # When they convert the input into frames,\n",
        "        # pretty_midi.get_piano_roll uses `ceil`,\n",
        "        # but torchaudio.transforms.melspectrogram uses `round`.\n",
        "        padded_audio = nn.functional.pad(audio, (HOP_SIZE // 2, 0), 'constant')\n",
        "        mel = self.melspectrogram(padded_audio)[:, :, 1:]\n",
        "        mel = mel.transpose(-1, -2)\n",
        "        mel = torch.log(torch.clamp(mel, min=1e-9))\n",
        "        return mel\n",
        "\n",
        "\n",
        "class ConvStack(nn.Module):\n",
        "    def __init__(self, n_mels, cnn_unit, fc_unit):\n",
        "        super().__init__()\n",
        "\n",
        "        # shape of input: (batch_size, 1 channel, frames, input_features)\n",
        "        self.cnn = nn.Sequential(\n",
        "            # layer 0\n",
        "            nn.Conv2d(1, cnn_unit, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_unit),\n",
        "            nn.ReLU(),\n",
        "            # layer 1\n",
        "            nn.Conv2d(cnn_unit, cnn_unit, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_unit),\n",
        "            nn.ReLU(),\n",
        "            # layer 2\n",
        "            nn.MaxPool2d((1, 2)),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(cnn_unit, cnn_unit * 2, (3, 3), padding=1),\n",
        "            nn.BatchNorm2d(cnn_unit * 2),\n",
        "            nn.ReLU(),\n",
        "            # layer 3\n",
        "            nn.MaxPool2d((1, 2)),\n",
        "            nn.Dropout(0.25),\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear((cnn_unit * 2) * (n_mels // 4), fc_unit),\n",
        "            nn.Dropout(0.5))\n",
        "\n",
        "    def forward(self, mel):\n",
        "        x = mel.unsqueeze(1)\n",
        "        x = self.cnn(x)\n",
        "        x = x.transpose(1, 2).flatten(-2)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transcriber(nn.Module):\n",
        "    def __init__(self, cnn_unit, fc_unit):\n",
        "        super().__init__()\n",
        "\n",
        "        self.melspectrogram = LogMelSpectrogram()\n",
        "\n",
        "        self.frame_conv_stack = ConvStack(N_MELS, cnn_unit, fc_unit)\n",
        "        self.frame_fc = nn.Linear(fc_unit, 88)\n",
        "\n",
        "        self.onset_conv_stack = ConvStack(N_MELS, cnn_unit, fc_unit)\n",
        "        self.onset_fc = nn.Linear(fc_unit, 88)\n",
        "\n",
        "    def forward(self, audio):\n",
        "        mel = self.melspectrogram(audio)\n",
        "\n",
        "        x = self.frame_conv_stack(mel)  # (B, T, C)\n",
        "        frame_out = self.frame_fc(x)\n",
        "\n",
        "        x = self.onset_conv_stack(mel)  # (B, T, C)\n",
        "        onset_out = self.onset_fc(x)\n",
        "        return frame_out, onset_out\n",
        "\n",
        "\n",
        "model = Transcriber(cnn_unit=64, fc_unit=256)\n",
        "\n",
        "with torch.no_grad():\n",
        "  out = model(batch['audio'])\n",
        "print(f'audio_input_shape: {batch[\"audio\"].shape}')\n",
        "print(f'frame_out_shape: {out[0].shape}')\n",
        "print(f'onset_out_shape: {out[1].shape}')\n",
        "print(f'frame_label_shape: {batch[\"frame\"].shape}')\n",
        "print(f'onset_label_shape: {batch[\"onset\"].shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rn1Xu_GrNA5"
      },
      "outputs": [],
      "source": [
        "# Train baseline model\n",
        "\n",
        "train(model_type='baseline',\n",
        "      logdir='runs/baseline_train',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=iterations,\n",
        "      validation_interval=validation_interval,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=False,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H6doBfTArNA5"
      },
      "source": [
        "# Problem 1: Implement RNN model\n",
        "- Implement ``Transcriber_RNN``\n",
        "  - Using ``nn.LSTM`` and ``nn.Linear``\n",
        "- You have to declare ``self.frame_lstm``, ``self.frame_fc``, ``self.onset_lstm``, ``self.onset_fc``\n",
        "  - Both LSTM layers has to be bidirectional, with number of layers of 2, and batch_first is True, hidden_size of ``fc_unit``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4LHI7xCrNA5"
      },
      "outputs": [],
      "source": [
        "class Transcriber_RNN(nn.Module):\n",
        "    def __init__(self, cnn_unit, fc_unit):\n",
        "        '''\n",
        "        TODO: Complete the initialization of the model.\n",
        "\n",
        "        Args:\n",
        "          cnn_unit: unit for number of channels in CNN Stack (You don't have to use this in Transcriber_RNN)\n",
        "          fc_unit: unit ofr number of hidden units in LSTM layer\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # Notice: Changing the initialization order may fail the tests.\n",
        "        self.melspectrogram = LogMelSpectrogram()\n",
        "\n",
        "        self.frame_lstm = ...\n",
        "        self.frame_fc = ...\n",
        "\n",
        "        self.onset_lstm = ...\n",
        "        self.onset_fc = ...\n",
        "\n",
        "\n",
        "    def forward(self, audio):\n",
        "        # TODO: Question 1\n",
        "        return frame_out, onset_out\n",
        "\n",
        "\n",
        "model = Transcriber_RNN(cnn_unit=64, fc_unit=256)\n",
        "\n",
        "out = model(batch['audio'])\n",
        "print(f'audio_input_shape: {batch[\"audio\"].shape}')\n",
        "print(f'frame_out_shape: {out[0].shape}')\n",
        "print(f'onset_out_shape: {out[1].shape}')\n",
        "print(f'frame_label_shape: {batch[\"frame\"].shape}')\n",
        "print(f'onset_label_shape: {batch[\"onset\"].shape}')\n",
        "\n",
        "# Test training with 100 iterations\n",
        "train(model_type='rnn',\n",
        "      logdir='runs/rnn_debug',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=100,\n",
        "      validation_interval=50,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=True,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v_lZluX5mDK"
      },
      "outputs": [],
      "source": [
        "# Train with 5000 iterations\n",
        "train(model_type='rnn',\n",
        "      logdir='runs/rnn_debug',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=iterations,\n",
        "      validation_interval=validation_interval,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=True,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w3lp4AfdrNA5"
      },
      "source": [
        "# Problem 2: Implement CRNN model\n",
        "- Implement ``Transcriber_CRNN``\n",
        "  - Using ``ConvStack``, ``nn.LSTM`` and ``nn.Linear``\n",
        "- You have to declare ``self.frame_conv_stack``,  ``self.frame_lstm``, ``self.frame_fc``, ``self.onset_conv_stack``,  ``self.onset_lstm``, ``self.onset_fc``\n",
        "  - Both LSTM layers has to be bidirectional, with number of layers of 1, and batch_first is True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAdzFA07rNA5"
      },
      "outputs": [],
      "source": [
        "class Transcriber_CRNN(nn.Module):\n",
        "    def __init__(self, cnn_unit, fc_unit):\n",
        "        '''\n",
        "        TODO: Complete the initialization of the model.\n",
        "\n",
        "        Args:\n",
        "          cnn_unit: unit for number of channels in CNN Stack\n",
        "          fc_unit: unit for number of hidden units in FC layer of CNN stack and LSTM layer\n",
        "        '''\n",
        "        super().__init__()\n",
        "        # Notice: Changing the initialization order may fail the tests.\n",
        "        self.melspectrogram = LogMelSpectrogram()\n",
        "\n",
        "        self.frame_conv_stack = ...\n",
        "        self.frame_lstm = ...\n",
        "        self.frame_fc = ...\n",
        "\n",
        "        self.onset_conv_stack = ...\n",
        "        self.onset_lstm = ...\n",
        "        self.onset_fc = ...\n",
        "\n",
        "    def forward(self, audio):\n",
        "        '''\n",
        "        TODO: Complete the forward pass of the model,\n",
        "\n",
        "        Args:\n",
        "          audio: torch.Tensor of shape (batch_size, sequence_length)\n",
        "\n",
        "        Returns:\n",
        "          frame_out: torch.Tensor of shape (batch_size, sequence_length, 88)\n",
        "          onset_out: torch.Tensor of shape (batch_size, sequence_length, 88)\n",
        "        '''\n",
        "\n",
        "        return frame_out, onset_out\n",
        "\n",
        "model = Transcriber_CRNN(cnn_unit=64, fc_unit=256)\n",
        "\n",
        "out = model(batch['audio'])\n",
        "print(f'audio_input_shape: {batch[\"audio\"].shape}')\n",
        "print(f'frame_out_shape: {out[0].shape}')\n",
        "print(f'onset_out_shape: {out[1].shape}')\n",
        "print(f'frame_label_shape: {batch[\"frame\"].shape}')\n",
        "print(f'onset_label_shape: {batch[\"onset\"].shape}')\n",
        "\n",
        "# Test training with 100 iterations\n",
        "train(model_type='crnn',\n",
        "      logdir='runs/crnn_debug',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=100,\n",
        "      validation_interval=50,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=True,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyGbWDTdrNA5"
      },
      "outputs": [],
      "source": [
        "# Train CRNN model\n",
        "# You can change the hyperparameters to get better results\n",
        "\n",
        "train(model_type='crnn',\n",
        "      logdir='runs/crnn_train',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=iterations,\n",
        "      validation_interval=validation_interval,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=False,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xZCsV5tQrNA5"
      },
      "source": [
        "# Problem 3: Implement Onsets and Frames model\n",
        "- Implement ``Transcriber_ONF``\n",
        "  - Using ``ConvStack``, ``nn.LSTM`` and ``nn.Linear``\n",
        "- You have to declare ``self.frame_conv_stack``,  ``self.frame_fc``, ``self.onset_lstm``, ``self.onset_fc``, ``self.combined_lstm``, ``self.combined_fc``\n",
        "  - For LSTM layers, you have to use bidirectional, num_layers 1, and batch_first True\n",
        "- Model Diagram (CAUTION: You should not output value after Sigmoid, because the loss function already includes Sigmoid layer. However, Onset Prediction to Frame Bi-LSTM should have Sigmoid activation)\n",
        "\n",
        "<img src=\"https://github.com/jdasam/ant5015-2023/blob/main/onsets_and_frames.jpg?raw=true\" alt=\"onsets and frames diagram\" width=50%>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2u5krqDrNA5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Transcriber_ONF(nn.Module):\n",
        "    def __init__(self, cnn_unit, fc_unit):\n",
        "        super().__init__()\n",
        "        '''\n",
        "        TODO: Complete the initialization of the model.\n",
        "\n",
        "        Args:\n",
        "          cnn_unit: unit for number of channels in CNN Stack\n",
        "          fc_unit: unit for number of hidden units in FC layer of CNN stack and LSTM layer\n",
        "        '''\n",
        "\n",
        "        # Notice: Changing the initialization order may fail the tests.\n",
        "        self.melspectrogram = LogMelSpectrogram()\n",
        "\n",
        "        self.frame_conv_stack = ...\n",
        "        self.frame_fc = ...\n",
        "\n",
        "        self.onset_conv_stack = ...\n",
        "        self.onset_lstm = ...\n",
        "        self.onset_fc = ...\n",
        "\n",
        "        self.combined_lstm = ...\n",
        "        self.combined_fc = ...\n",
        "\n",
        "\n",
        "    def forward(self, audio):\n",
        "        '''\n",
        "        # TODO: implement this function based on the given diagram\n",
        "\n",
        "        Args:\n",
        "          audio: torch.Tensor of shape (batch_size, sequence_length)\n",
        "\n",
        "        Returns:\n",
        "          frame_out: torch.Tensor of shape (batch_size, sequence_length, 88)\n",
        "          onset_out: torch.Tensor of shape (batch_size, sequence_length, 88)\n",
        "\n",
        "        CAUTION: You should not apply Sigmoid or Softmax for the output of the model\n",
        "                !!BUT!! Onset Prediction to Frame Bi-LSTM should have Sigmoid activation\n",
        "        '''\n",
        "\n",
        "        return frame_out, onset_out\n",
        "\n",
        "\n",
        "model = Transcriber_ONF(cnn_unit=64, fc_unit=256)\n",
        "\n",
        "out = model(batch['audio'])\n",
        "print(f'audio_input_shape: {batch[\"audio\"].shape}')\n",
        "print(f'frame_out_shape: {out[0].shape}')\n",
        "print(f'onset_out_shape: {out[1].shape}')\n",
        "print(f'frame_label_shape: {batch[\"frame\"].shape}')\n",
        "print(f'onset_label_shape: {batch[\"onset\"].shape}')\n",
        "\n",
        "\n",
        "# Test training with 100 iterations\n",
        "train(model_type='ONF',\n",
        "      logdir='runs/onf_debug',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=100,\n",
        "      validation_interval=50,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=True,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIQTTxm0rNA6"
      },
      "outputs": [],
      "source": [
        "train(model_type='ONF',\n",
        "      logdir='runs/onf_train',\n",
        "      dataset=dataset,\n",
        "      valid_dataset=validation_dataset,\n",
        "      batch_size=batch_size,\n",
        "      iterations=iterations,\n",
        "      validation_interval=validation_interval,\n",
        "      sequence_length=sequence_length,\n",
        "      learning_rate=lr,\n",
        "      weight_decay=weight_decay,\n",
        "      cnn_unit=cnn_unit,\n",
        "      fc_unit=fc_unit,\n",
        "      debug=False,\n",
        "      save_midi=False\n",
        "      )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5tve-r3grNA6"
      },
      "source": [
        "# Inference\n",
        "- Transcribe an audio file of your choice\n",
        "  - The recording has to be solo piano performance\n",
        "- Describe the result of your transcription\n",
        "  - When does the model make mistakes?\n",
        "  - What are the limitations of the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roaBHryXrNA6"
      },
      "outputs": [],
      "source": [
        "def load_audio(audiofile):\n",
        "    audio, sr = torchaudio.load(audiofile)\n",
        "    audio = torchaudio.transforms.Resample(sr, SAMPLE_RATE)(audio)\n",
        "    return audio.mean(dim=0)\n",
        "\n",
        "\n",
        "def transcribe(audio, model, save_name, max_len, device='cuda'):\n",
        "    print(f'save_path: {save_name}')\n",
        "    audio = audio[:max_len * SAMPLE_RATE]\n",
        "    t_audio = audio.to(torch.float).to(device)\n",
        "    pad_len = math.ceil(len(t_audio) / HOP_SIZE) * HOP_SIZE - len(t_audio)\n",
        "    t_audio = torch.unsqueeze(F.pad(t_audio, (0, pad_len)), 0)\n",
        "\n",
        "    frame_logit, onset_logit = model(t_audio)\n",
        "    onset = torch.sigmoid(onset_logit[0])\n",
        "    frame = torch.sigmoid(frame_logit[0])\n",
        "\n",
        "    p_est, i_est = extract_notes(onset, frame)\n",
        "\n",
        "    scaling = HOP_SIZE / SAMPLE_RATE\n",
        "\n",
        "    i_est = (i_est * scaling).reshape(-1, 2)\n",
        "    p_est = np.array([midi_to_hz(MIN_MIDI + pitch) for pitch in p_est])\n",
        "\n",
        "    np_filename = Path(save_name).parent / (Path(save_name).stem + '.npz')\n",
        "    np.savez(np_filename, onset=onset.cpu().numpy(), frame=frame.cpu().numpy())\n",
        "\n",
        "    midi_filename = Path(save_name).parent / (Path(save_name).stem + '.midi')\n",
        "    save_midi(midi_filename, p_est, i_est, [64] * len(p_est))\n",
        "\n",
        "    wav_filename = Path(save_name).parent / (Path(save_name).stem + '.wav')\n",
        "    midi_file = pretty_midi.PrettyMIDI(str(midi_filename))\n",
        "    synth_audio = midi_file.fluidsynth(fs=16000)\n",
        "    soundfile.write(wav_filename, synth_audio, 16000)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvdy8KH72Rlv"
      },
      "outputs": [],
      "source": [
        "# simple test file: monophonic piano scale\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://www.audiolabs-erlangen.de/resources/MIR/FMP/data/C3/FMP_C3_F03.mp3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AxyFrDerNA6"
      },
      "outputs": [],
      "source": [
        "model_state_path = ... # path to the model state file. it is named like 'runs/baseline_train/model-5000.pt'\n",
        "audio_file_path = ...# path to the audio file.\n",
        "max_len = 30  # seconds\n",
        "\n",
        "model_state_path = 'runs/baseline_train/model-5000.pt'\n",
        "audio_file_path = 'FMP_C3_F03.mp3'\n",
        "DEV = 'cuda'\n",
        "\n",
        "\n",
        "with torch.inference_mode():\n",
        "    ckp = torch.load(model_state_path, map_location='cpu')\n",
        "    model = Transcriber(ckp['cnn_unit'], ckp['fc_unit'])\n",
        "\n",
        "    model.load_state_dict(ckp['model_state_dict'])\n",
        "    model.eval()\n",
        "    model = model.to(DEV)\n",
        "\n",
        "    audio = load_audio(audio_file_path)\n",
        "\n",
        "    save_path = Path(model_state_path).parent / (\n",
        "            Path(audio_file_path).stem + '_transcribed')\n",
        "\n",
        "    transcribe(audio, model, save_path, max_len, device=DEV)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
