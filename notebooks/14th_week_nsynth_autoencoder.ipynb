{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/ant5015/blob/main/notebooks/14th_week_nsynth_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfgcgtHWXBhv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_otZoi_XBhx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Let's only use testset\n",
        "'''\n",
        "!wget http://download.magenta.tensorflow.org/datasets/nsynth/nsynth-test.jsonwav.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU-zfoIcXBhx"
      },
      "outputs": [],
      "source": [
        "!tar -xf nsynth-test.jsonwav.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znh04KSGXBhy"
      },
      "outputs": [],
      "source": [
        "class NSynthDataSet:\n",
        "  def __init__(self, path):\n",
        "    if isinstance(path, str):\n",
        "      path = Path(path)\n",
        "    self.path = path\n",
        "    json_path = path / \"examples.json\"\n",
        "    # self.meta = pd.read_json(json_path).to_dict()\n",
        "    self.meta= pd.read_json(json_path)\n",
        "    self.file_list = list(self.path.rglob('*.wav'))\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    fn = self.file_list[idx]\n",
        "    audio, sr = torchaudio.load(fn)\n",
        "    pitch = self.meta[fn.stem]['pitch']\n",
        "    pitch = torch.tensor(pitch, dtype=torch.long)\n",
        "    return audio, pitch\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.meta.keys())\n",
        "\n",
        "dataset = NSynthDataSet(Path('nsynth-test'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXdxaIwyXBhy"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
        "# test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=4)\n",
        "batch = next(iter(train_loader))\n",
        "audio, pitch = batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhWak6SAXBhy"
      },
      "source": [
        "## NSynth Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahATi33QXBhz"
      },
      "outputs": [],
      "source": [
        "class SpecModel(nn.Module):\n",
        "  def __init__(self, n_fft, hop_length):\n",
        "    super().__init__()\n",
        "    self.spec_converter = torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length)\n",
        "    self.db_converter = torchaudio.transforms.AmplitudeToDB(stype='power')\n",
        "\n",
        "  def forward(self, audio_sample):\n",
        "    spec = self.spec_converter(audio_sample)\n",
        "    db_spec = self.db_converter(spec)\n",
        "    return db_spec\n",
        "\n",
        "class Conv2dNormPool(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "    self.batch_norm = nn.BatchNorm2d(out_channels)\n",
        "    self.activation = nn.LeakyReLU(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.batch_norm(x)\n",
        "    x = self.activation(x)\n",
        "    return x\n",
        "\n",
        "class Conv2dNormTransposePool(Conv2dNormPool):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n",
        "    super().__init__(in_channels, out_channels, kernel_size, padding, stride)\n",
        "    self.conv = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n",
        "\n",
        "\n",
        "class AutoEncoder(nn.Module):\n",
        "  def __init__(self, n_fft, hop_length, hidden_size=256):\n",
        "    super().__init__()\n",
        "    self.spec_model = SpecModel(n_fft, hop_length)\n",
        "    self.encoder = nn.Sequential()\n",
        "    self.pitch_embedder = nn.Embedding(121, hidden_size//2)\n",
        "    self.num_channels = [1] + [128] * 3 + [256] * 3 + [512] * 2 + [1024]\n",
        "    i = 0\n",
        "    self.encoder.add_module(f\"conv_norm{i}\", Conv2dNormPool(self.num_channels[i], self.num_channels[i+1], (5,5), 2, (2,2) ))\n",
        "    for i in range(1,7):\n",
        "      self.encoder.add_module(f\"conv_norm{i}\", Conv2dNormPool(self.num_channels[i], self.num_channels[i+1], (4,4), 1, (2,2) ))\n",
        "    i = 7\n",
        "    self.encoder.add_module(f\"conv_norm{i}\", Conv2dNormPool(self.num_channels[i], self.num_channels[i+1], (2,2), 0, (2,2) ))\n",
        "    i = 8\n",
        "    self.encoder.add_module(f\"conv_norm{i}\", Conv2dNormPool(self.num_channels[i], self.num_channels[i+1], (1,1), 0, (1,1) ))\n",
        "    self.final_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
        "    self.decoder = nn.Sequential(\n",
        "        Conv2dNormTransposePool(in_channels=self.num_channels[-1] + hidden_size//2, out_channels=self.num_channels[-2], kernel_size=(2,1), padding=0, stride=(2,2))\n",
        "    )\n",
        "    i = 0\n",
        "    self.decoder.add_module(f\"conv_norm{i}\", Conv2dNormTransposePool(self.num_channels[-2-i], self.num_channels[-3-i], (2,2), 0, (2,2)))\n",
        "    for i in range(1,7):\n",
        "      self.decoder.add_module(f\"conv_norm{i}\", Conv2dNormTransposePool(self.num_channels[-2-i], self.num_channels[-3-i], (4,4), 1, (2,2)))\n",
        "    self.decoder.add_module(\"final_module\",  nn.ConvTranspose2d(in_channels=self.num_channels[1], out_channels=1, kernel_size=(4,4), padding=1, stride=(2,2)),)\n",
        "\n",
        "\n",
        "  def forward(self, x, pitch):\n",
        "    spec = self.spec_model(x)\n",
        "    spec = spec[:,:,:-1] # to match 512\n",
        "    spec /= 80\n",
        "    spec = nn.functional.pad(spec, (2,3), value=torch.min(spec))\n",
        "    out = self.encoder(spec)\n",
        "\n",
        "    latent = self.final_layer(out.view(out.shape[0], -1))\n",
        "    latent = torch.cat([latent, self.pitch_embedder(pitch)], dim=-1)\n",
        "    latent = latent.view(latent.shape[0], -1, 1, 1)\n",
        "    recon_spec = self.decoder(latent)\n",
        "    return recon_spec, spec\n",
        "\n",
        "model = AutoEncoder(1024, 256, 1024)\n",
        "recon_spec, spec = model(audio, pitch)\n",
        "recon_spec.shape, spec.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dn9zRFCXBhz"
      },
      "source": [
        "## Download pretrained model\n",
        "- The model was trained about 1800k iterations with entire training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OHFLLtiXBhz"
      },
      "outputs": [],
      "source": [
        "pretrained_weights = torch.load('autoencoder_last.pt', map_location='cpu')\n",
        "model.load_state_dict(pretrained_weights)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ7zL7uLXBhz"
      },
      "outputs": [],
      "source": [
        "test_loader = DataLoader(dataset, batch_size=64, num_workers=4,pin_memory=True)\n",
        "test_batch = next(iter(test_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lZEyD8VXBhz"
      },
      "source": [
        "### Convert Spectrogram to Wav using Griffin-Lim Algorithm\n",
        "- The model is trained to generate magnitude spectrogram, so we need to convert it to wav file using Griffin-Lim Algorithm\n",
        "- Griffin-Lim Algorithm is an iterative algorithm to estimate phase information from magnitude spectrogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an1ucXZjXBhz"
      },
      "outputs": [],
      "source": [
        "def network_output_to_audio(spec):\n",
        "  rescaled_spec = spec * 80\n",
        "  padded_spec = nn.functional.pad(rescaled_spec, (0,0, 0,1), value=-100)\n",
        "  magnitude_spec = torchaudio.functional.DB_to_amplitude(padded_spec, ref=1, power=1)\n",
        "  griffin_lim = torchaudio.transforms.GriffinLim(n_fft=1024, hop_length=256, n_iter=100)\n",
        "  spec_recon_audio = griffin_lim(magnitude_spec)\n",
        "\n",
        "  return spec_recon_audio\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}