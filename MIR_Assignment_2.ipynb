{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DJvoIiAUj6P"
   },
   "source": [
    "# Assignment 2: Music Auto-tagging Model\n",
    "- In this assignment, you will train your auto-tagging model using PyTorch\n",
    "- The dataset is from MagnaTagATune\n",
    "  - Randomly selected 8000 mp3 files\n",
    "  - 5000 files for training, 1000 for validation, 2000 for test  \n",
    "- Every code cell before the Problem 0 has to be ran without modification or error\n",
    "- You have to submit three files:\n",
    "  - Report in PDF (free format)\n",
    "      - Explain what you have tried and what you have got as the result\n",
    "      - Explain briefly of each code you wrote\n",
    "  - Notebook in ipynb\n",
    "  - Audio data that you used for Problem 7\n",
    " \n",
    "- Problem 1: Complete Three Dataset Classes (21 pts)\n",
    "- Problem 2: Practice with nn.Sequential() (5 pts)\n",
    "- Problem 3: Make Your Own Conv Layers (10 pts)\n",
    "- Probelm 4: Try Various Settings and Report (25 pts)\n",
    "- Problem 5: Complete Binary Cross Entropy Function (4 pts)\n",
    "- Problem 6: Complete Precision-Recall Area Under Curve Function (20 pts)\n",
    "- Problem 7: Load audio and make prediction (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XENajfe8Uj6U"
   },
   "outputs": [],
   "source": [
    "DEV = 'cuda' # select your device 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKLjUzcbUj6W"
   },
   "source": [
    "## 0. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5LcyB2rUj6W",
    "outputId": "23958e04-69d3-4080-f143-f4b802691aaf"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "def save_fig_with_date(figname:str):\n",
    "  plt.savefig(f\"{figname}_{datetime.now().strftime('%m_%d_%H_%M_%S')}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aiocvB4v8jCC"
   },
   "source": [
    "- Download dataset from Google Drive link and Unzip at `MTAT_SMALL/`\n",
    "  - You can also download it from [OneDrive Link](https://sogang365-my.sharepoint.com/:u:/g/personal/dasaem_jeong_o365_sogang_ac_kr/EdkHWV-qvxBEi-d0Ua73VG4BEp7EZO7HMvrXsWqeJvMJzg?e=Yi4jf0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEjKpTS_WCu4"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade gdown\n",
    "!gdown --id 15e9E3oZdudErkPKwb0rCAiZXkPxdZkV6\n",
    "!unzip -q mtat_8000.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOSCuOJMUj6W"
   },
   "source": [
    "## Problem 1. Complete Dataset Class (21 pts)\n",
    "- In this problem, you have to implement three ways to load the data\n",
    "    - 1) Load audio file and resample every time the data is called \n",
    "    - 2) Save pre-processed data in .pt file and load it every time the data is called \n",
    "    - 3) Load every audio file on memory before the training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T_lpbZK1Uj6X"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "You don't have to change this cell\n",
    "'''\n",
    "class MTATDataset:\n",
    "  def __init__(self, dir_path:str, split:str='train', num_max_data:int=4000, sr:int=16000):\n",
    "    self.dir = Path(dir_path)\n",
    "    self.labels = pd.read_csv(self.dir / \"meta.csv\", index_col=[0])\n",
    "    self.sr = sr\n",
    "\n",
    "    if split==\"train\":\n",
    "      sub_dir_ids = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c']\n",
    "    elif split=='valid':\n",
    "      sub_dir_ids = ['d']\n",
    "    else: #test\n",
    "      sub_dir_ids = ['e', 'f', 'g']\n",
    "\n",
    "    is_in_set = [True if x[0] in sub_dir_ids else False for x in self.labels['mp3_path'].values.astype('str')]\n",
    "    self.labels = self.labels.iloc[is_in_set]\n",
    "    self.labels = self.labels[:num_max_data]\n",
    "    self.vocab = self.labels.columns.values[1:-1]\n",
    "    self.label_tensor = self.convert_label_to_tensor()\n",
    "  \n",
    "  def convert_label_to_tensor(self):\n",
    "    return torch.LongTensor(self.labels.values[:, 1:-1].astype('bool'))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.labels)\n",
    "  \n",
    "\n",
    "MTAT_DIR = Path('MTAT_SMALL/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clip_id</th>\n",
       "      <th>singer</th>\n",
       "      <th>harpsichord</th>\n",
       "      <th>sitar</th>\n",
       "      <th>heavy</th>\n",
       "      <th>foreign</th>\n",
       "      <th>no piano</th>\n",
       "      <th>classical</th>\n",
       "      <th>female</th>\n",
       "      <th>jazz</th>\n",
       "      <th>...</th>\n",
       "      <th>rock</th>\n",
       "      <th>dance</th>\n",
       "      <th>cello</th>\n",
       "      <th>techno</th>\n",
       "      <th>flute</th>\n",
       "      <th>beat</th>\n",
       "      <th>soft</th>\n",
       "      <th>choir</th>\n",
       "      <th>baroque</th>\n",
       "      <th>mp3_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20552</th>\n",
       "      <td>45147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2/zephyrus-angelus-11-ave_maria__virgo_serena_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>8539</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a/tilopa-pictures_of_silence-02-ni-175-204.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8996</th>\n",
       "      <td>19647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5/arthur_yoria-of_the_lovely-04-several_mistak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4055</th>\n",
       "      <td>8856</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8/stargarden-music_for_modern_listening-02-per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6361</th>\n",
       "      <td>13834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a/dac_crowell-the_mechanism_of_starlight-03-me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15397</th>\n",
       "      <td>33729</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4/jami_sieber-second_sight-07-the_goats_earth-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19285</th>\n",
       "      <td>42374</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9/self_delusion-happiness_hurts_me-10-dead_sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>8934</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c/jamie_janover-now_center_of_time-02-playa-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18897</th>\n",
       "      <td>41453</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2/jesse_manno-sea_spirits-09-tidur-59-88.mp3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5813</th>\n",
       "      <td>12758</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6/philharmonia_baroque-rameau_and_leclair-03-e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       clip_id  singer  harpsichord  sitar  heavy  foreign  no piano  \\\n",
       "20552    45147       0            0      0      0        0         0   \n",
       "3899      8539       0            0      0      0        0         0   \n",
       "8996     19647       0            0      0      0        0         0   \n",
       "4055      8856       0            0      0      0        0         0   \n",
       "6361     13834       0            0      0      0        0         0   \n",
       "...        ...     ...          ...    ...    ...      ...       ...   \n",
       "15397    33729       0            0      0      0        0         0   \n",
       "19285    42374       0            0      0      0        0         0   \n",
       "4099      8934       0            0      1      0        0         0   \n",
       "18897    41453       0            0      0      0        0         0   \n",
       "5813     12758       0            0      0      0        0         0   \n",
       "\n",
       "       classical  female  jazz  ...  rock  dance  cello  techno  flute  beat  \\\n",
       "20552          0       1     0  ...     0      0      0       0      0     0   \n",
       "3899           0       0     0  ...     0      0      0       0      0     0   \n",
       "8996           0       0     0  ...     0      0      0       0      0     0   \n",
       "4055           0       0     0  ...     0      0      0       0      0     0   \n",
       "6361           1       0     0  ...     0      0      0       0      0     0   \n",
       "...          ...     ...   ...  ...   ...    ...    ...     ...    ...   ...   \n",
       "15397          0       0     0  ...     0      0      0       0      0     0   \n",
       "19285          0       0     0  ...     0      0      0       0      0     0   \n",
       "4099           0       0     0  ...     0      0      0       0      0     0   \n",
       "18897          0       0     0  ...     0      0      0       0      1     0   \n",
       "5813           1       0     0  ...     0      0      0       0      1     0   \n",
       "\n",
       "       soft  choir  baroque                                           mp3_path  \n",
       "20552     0      1        0  2/zephyrus-angelus-11-ave_maria__virgo_serena_...  \n",
       "3899      0      0        0     a/tilopa-pictures_of_silence-02-ni-175-204.mp3  \n",
       "8996      0      0        0  5/arthur_yoria-of_the_lovely-04-several_mistak...  \n",
       "4055      0      0        0  8/stargarden-music_for_modern_listening-02-per...  \n",
       "6361      0      0        0  a/dac_crowell-the_mechanism_of_starlight-03-me...  \n",
       "...     ...    ...      ...                                                ...  \n",
       "15397     0      0        0  4/jami_sieber-second_sight-07-the_goats_earth-...  \n",
       "19285     0      0        0  9/self_delusion-happiness_hurts_me-10-dead_sta...  \n",
       "4099      0      0        0  c/jamie_janover-now_center_of_time-02-playa-20...  \n",
       "18897     0      0        0       2/jesse_manno-sea_spirits-09-tidur-59-88.mp3  \n",
       "5813      0      0        1  6/philharmonia_baroque-rameau_and_leclair-03-e...  \n",
       "\n",
       "[4000 rows x 52 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Check how baseline dataset looks like\n",
    "'''\n",
    "\n",
    "base_set = MTATDataset(MTAT_DIR)\n",
    "\n",
    "'''\n",
    "metadata of dataset is stored in self.labels\n",
    "'''\n",
    "base_set.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You can use labels['mp3_path'].iloc\n",
    "'''\n",
    "target_idx = 0 \n",
    "\n",
    "path_to_target_idx = base_set.labels['mp3_path'].iloc[target_idx]\n",
    "print(path_to_target_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 1, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 1,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "label of each tensor is also stored in self.label_tensor\n",
    "'''\n",
    "base_set.label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnTheFlyDataset(MTATDataset):\n",
    "  def __init__(self, dir_path:str, split:str='train', num_max_data:int=4000, sr:int=16000):\n",
    "    super().__init__(dir_path, split, num_max_data, sr)\n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
    "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
    "    \n",
    "    OnTheFlyDataset loads the audio file whenever this __getitem__ function is called.\n",
    "    In this function, you have to implement these things\n",
    "    \n",
    "    1) Get the file path of idx-th data sample (use self.labels['mp3_path'])\n",
    "    2) Load the audio of that file path\n",
    "    3) Resample the audio sample into frequency of self.sr (You can use torchaudio.functional.resample)\n",
    "    4) Return resampled audio sample and the label (tag data) of the data sample\n",
    "    \n",
    "    Output\n",
    "      audio_sample (torch.FloatTensor):  \n",
    "      label (torch.FloatTensor): A tensor with shape of 50 dimension. Each dimension has value either 0 or 1\n",
    "                                 If n-th dimension's value is 1, it means n-th tag is True for this data sample\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    audio_sample = None\n",
    "    label = None\n",
    "    \n",
    "    return audio_sample, label\n",
    "\n",
    "dummy_set = OnTheFlyDataset(MTAT_DIR, split='train', num_max_data=100)\n",
    "audio, label = dummy_set[1]\n",
    "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
    "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
    "print(dummy_set.vocab[torch.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessDataset(MTATDataset):\n",
    "  def __init__(self, dir_path:str, split:str='train', num_max_data:int=4000, sr:int=16000):\n",
    "    super().__init__(dir_path, split, num_max_data, sr)\n",
    "    \n",
    "    self.pre_process_and_save_data()\n",
    "    \n",
    "  def pre_process_and_save_data(self):\n",
    "    '''\n",
    "    self.pre_process_and_save_data loads every audio sample in the dataset, resample it, and save it into pt file.\n",
    "    In this function, you have to implement these things\n",
    "    \n",
    "    1) For every data sample in the dataset, check whether pre-processed data already exists\n",
    "      - You can get data sample path by self.labels['mp3_path'].values\n",
    "      - path of pre-processed data can be in the same directory, but with different suffix.\n",
    "      - You can make it with Path(mp3_path).with_suffix('.pt')\n",
    "    2) If it doesn't exist, do follow things\n",
    "      a) Load audio file \n",
    "      b) Resample the audio file with samplerate of self.sr\n",
    "      c) Get label of this audio file\n",
    "      d) Save {'audio': audio_tensor, 'label':label_tensor} with torch.save\n",
    "    \n",
    "    Output\n",
    "      None\n",
    "    \n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    \n",
    "    \n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
    "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
    "    \n",
    "    PreProcessDataset loads the pre-processed pt file whenever this __getitem__ function is called.\n",
    "    In this function, you have to implement these things\n",
    "    \n",
    "    1) Get the pt file path of idx-th data sample (use self.labels)\n",
    "    2) Load the pre-procssed data of that file path (use torch.load)\n",
    "    3) Return the audio sample and the label (tag data) of the data sample\n",
    "\n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "\n",
    "    return\n",
    "  \n",
    "dummy_set = PreProcessDataset(MTAT_DIR, split='train', num_max_data=100)\n",
    "audio, label = dummy_set[15]\n",
    "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
    "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
    "print(dummy_set.vocab[torch.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class OnMemoryDataset(MTATDataset):\n",
    "  def __init__(self, dir_path:str, split:str='train', num_max_data:int=4000, sr:int=16000):\n",
    "    super().__init__(dir_path, split, num_max_data, sr)\n",
    "    \n",
    "    self.loaded_audios = self.load_audio()\n",
    "    \n",
    "  def load_audio(self):\n",
    "    '''\n",
    "    In this function, you have to load all the audio file in the dataset, and resample them, \n",
    "    and store the data on the memory as a python variable\n",
    "    \n",
    "    For each data in the dataset,\n",
    "      a) Load Audio\n",
    "      b) Resample it to self.sr\n",
    "      c) Append it to total_audio_datas\n",
    "    \n",
    "    Output:\n",
    "      total_audio_datas (list): A list of torch.FloatTensor. i-th item of the list corresponds to the audio sample of i-th data\n",
    "                                Each item is an audio sample in torch.FloatTensor with sampling rate of self.sr \n",
    "    '''\n",
    "    total_audio_datas = []\n",
    "    \n",
    "    ### Write your code from here\n",
    "    return total_audio_datas\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    '''\n",
    "    __getitem__ returns a corresponding idx-th data sample among the dataset.\n",
    "    In music-tag dataset, it has to return (audio_sample, label) of idx-th data.\n",
    "    \n",
    "    OnMemoryDataset returns the pre-loaded audio data that is aved on self.loaded_audios whenever this __getitem__ function is called.\n",
    "    In this function, you have to implement these things\n",
    "    \n",
    "    1) Load the pre-procssed audio data from self.loaded_audios\n",
    "    2) Return the audio sample and the label (tag data) of the data sample\n",
    "\n",
    "    TODO: Complete this function\n",
    "    '''\n",
    "    \n",
    "    return\n",
    "  \n",
    "dummy_set = OnMemoryDataset(MTAT_DIR, split='train', num_max_data=50)\n",
    "audio, label = dummy_set[10]\n",
    "assert audio.ndim == 1, \"Number of dimensions of audio tensor has to be 1. Use audio[0] or audio.mean(dim=0) to reduce it\"\n",
    "ipd.display(ipd.Audio(audio, rate=dummy_set.sr))\n",
    "print(dummy_set.vocab[torch.where(label)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Dataset\n",
    "- You can select one of your implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_dataset_class = PreProcessDataset # One of OnTheFlyDataset, PreProcessDataset, or OnMemoryDataset\n",
    "# your_dataset_class = OnMemoryDataset\n",
    "'''\n",
    "Based on your memory size or storage size, you can change the num_max_data\n",
    "'''\n",
    "trainset = your_dataset_class(MTAT_DIR, split='train', num_max_data=5000)\n",
    "validset = your_dataset_class(MTAT_DIR, split='valid', num_max_data=1000)\n",
    "testset = your_dataset_class(MTAT_DIR, split='test', num_max_data=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIPU7DxUUj6Y"
   },
   "source": [
    "#### DataLoader\n",
    "- Define `DataLoader` using the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPauowdkUj6Z"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=0) # you can speed up with num_workers=4 if you have multiple cpu core\n",
    "valid_loader = DataLoader(validset, batch_size=128, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)\n",
    "\n",
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2kAXFFZUj6a"
   },
   "source": [
    "## Preparation: Define Neural Network\n",
    "- Define the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TB0nTbt7Uj6a"
   },
   "outputs": [],
   "source": [
    "class SpecModel(nn.Module):\n",
    "  def __init__(self, sr:int, n_fft:int, hop_length:int, n_mels:int):\n",
    "    super().__init__()\n",
    "    self.mel_converter = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    self.db_converter = torchaudio.transforms.AmplitudeToDB()\n",
    "  \n",
    "  def forward(self, x):\n",
    "    mel_spec = self.mel_converter(x)\n",
    "    return self.db_converter(mel_spec)\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "  def __init__(self, sr:int, n_fft:int, hop_length:int, n_mels:int, hidden_size:int, num_output:int):\n",
    "    super().__init__()\n",
    "    self.sr = sr\n",
    "    self.spec_converter = SpecModel(sr, n_fft, hop_length, n_mels)\n",
    "    self.conv_layer = nn.Sequential(\n",
    "      nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),\n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),     \n",
    "      nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "      nn.MaxPool1d(3),\n",
    "      nn.ReLU(),\n",
    "    )\n",
    "    self.final_layer = nn.Linear(hidden_size, num_output)\n",
    "\n",
    "  def get_spec(self, x):\n",
    "    '''\n",
    "    Get result of self.spec_converter\n",
    "    x (torch.Tensor): audio samples (num_batch_size X num_audio_samples)\n",
    "    '''\n",
    "    return self.spec_converter(x)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    spec = self.get_spec(x) # num_batch X num_mel_bins X num_time_bins\n",
    "    out = self.conv_layer(spec)\n",
    "    out = torch.max(out, dim=-1)[0] # select [0] because torch.max outputs tuple of (value, index)\n",
    "    out = self.final_layer(out)\n",
    "    out = torch.sigmoid(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0UIl_uvUj6b"
   },
   "source": [
    "## 3. Train the Network\n",
    "- First, just run the cells below so that you can obtain the first result\n",
    "- Plot the training loss and validation accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VBnZiX7Uj6b"
   },
   "outputs": [],
   "source": [
    "def get_tpr_fpr(pred:torch.Tensor, target:torch.Tensor, threshold:float=0.5):\n",
    "  thresh_pred = pred> threshold\n",
    "  p = torch.sum(target == 1)\n",
    "  tp = torch.sum((thresh_pred==1) * (target==1))\n",
    "  n = torch.sum(target == 0)\n",
    "  fp = torch.sum((thresh_pred==1) * (target==0))\n",
    "  return tp/p, fp/n\n",
    "\n",
    "def get_roc_auc(pred:torch.Tensor, target:torch.Tensor, num_grid=500):\n",
    "  auc = 0\n",
    "  prev_fpr = 0\n",
    "  for thresh in reversed(torch.linspace(0,1,num_grid)):\n",
    "    tpr, fpr = get_tpr_fpr(pred, label, threshold=thresh)\n",
    "    auc += tpr * (fpr-prev_fpr)\n",
    "    prev_fpr = fpr\n",
    "  return auc\n",
    "\n",
    "def train_model(model:nn.Module, train_loader:DataLoader, valid_loader:DataLoader, optimizer:torch.optim.Optimizer, num_epochs:int, loss_func:function, device='cuda'):\n",
    "  loss_records =[] \n",
    "  valid_acc_records = []\n",
    "  model.vocab = train_loader.dataset.vocab\n",
    "  model.train() # Set model to train mode\n",
    "  for epoch in tqdm(range(num_epochs)):\n",
    "    for batch in train_loader:\n",
    "      optimizer.zero_grad() # Rest gradient of every parameters in optimizer (every parameters in the model)\n",
    "      audio, label = batch\n",
    "      audio = audio.to(device)\n",
    "      label = label.to(device)\n",
    "      pred = model(audio)\n",
    "      loss = loss_func(pred, label.float())\n",
    "      loss.backward() # Run backpropagation\n",
    "      optimizer.step() # Update parameters\n",
    "      loss_records.append(loss.item())\n",
    "    valid_acc = validate_model(model, valid_loader, device)\n",
    "    valid_acc_records.append(valid_acc.item())\n",
    "  return {\"loss\": loss_records, \"valid_acc\": valid_acc_records}\n",
    "\n",
    "def validate_model(model, valid_loader, device, acc_func=get_roc_auc):\n",
    "  valid_acc = 0\n",
    "  model.eval()\n",
    "  model.to(device)\n",
    "  with torch.no_grad():\n",
    "    for batch in valid_loader:\n",
    "      audio, label = batch\n",
    "      pred = model(audio.to(device))\n",
    "      auc = acc_func(pred, label.to(device))\n",
    "      valid_acc += auc * len(label)\n",
    "  model.train()\n",
    "  return valid_acc / len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTHIVXvNUj6c",
    "outputId": "1f9d2e84-ad4c-4e50-b7b7-031fd6785620",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Train the default model\n",
    "'''\n",
    "\n",
    "model = AudioModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model = model.to(DEV)\n",
    "loss_func = torch.nn.BCELoss()\n",
    "train_record = train_model(model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUCYjyeyUj6c"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_record['loss'])\n",
    "save_fig_with_date('default_train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJ92F5T1Uj6c"
   },
   "outputs": [],
   "source": [
    "plt.plot(train_record['valid_acc'])\n",
    "save_fig_with_date('default_train_valid_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXnirpn4Uj6d"
   },
   "source": [
    "### Problem 2. Practice with nn.Sequential() (5 pts)\n",
    "- `nn.Sequential` automatically stacks the `nn.Module`\n",
    "    - If `x = nn.Sequential( nn.Conv1d(48, 12) , nn.ReLU() )`,\n",
    "        - `out = x(input)` is same with `out` below\n",
    "        - `x1 = nn.Conv1d(48,12)`, `x2=nn.ReLU()`, `out = x2(x1(input))`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgkwalESUj6d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class StackManualLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layer1 = nn.Conv1d(16, 4, kernel_size=2)\n",
    "    self.activation = nn.Sigmoid()\n",
    "    self.layer2 = nn.Conv1d(4, 4, kernel_size=2)\n",
    "    self.layer3 = nn.Conv1d(4, 1, kernel_size=2)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    out = self.layer1(x)\n",
    "    out = self.activation(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.activation(out)\n",
    "    out = self.layer3(out)\n",
    "    return out\n",
    "\n",
    "class SequentialLayer(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      '''\n",
    "      TODO: Complete this nn.Sequential so that it computes exactly same thing with StackManualLayer\n",
    "      '''\n",
    "    )\n",
    "  def forward(self, x):\n",
    "    out = self.layers(x)\n",
    "    return out\n",
    "  \n",
    "# Do not change the code below\n",
    "torch.manual_seed(0)\n",
    "manual_layer = StackManualLayer()\n",
    "torch.manual_seed(0)\n",
    "sequential_layer = SequentialLayer()\n",
    "\n",
    "'''\n",
    "The printed result has to be same\n",
    "'''\n",
    "\n",
    "test_dummy = torch.arange(128).view(1,16,8).float()\n",
    "manual_out = manual_layer(test_dummy)\n",
    "print(f\"Output with Manual Stack Layer: {manual_out}\")\n",
    "sequential_out = sequential_layer(test_dummy)\n",
    "print(f\"Output with Sequential Layer: {sequential_out}\")\n",
    "\n",
    "assert torch.allclose(manual_out, sequential_out), \"The output of manual layer and sequential layer is different\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1O9g9pliUj6d"
   },
   "source": [
    "### Problem 3. Make Your Own Conv Layers (15 pts)\n",
    "- Complete the `self.conv_layer` of `YourModel`\n",
    "- Train the model and compare the result\n",
    "- In your report, explain your conv_layer and the training result\n",
    "    - How did you change the structure of CNN?\n",
    "    - What is the difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIJnkBrRUj6e"
   },
   "outputs": [],
   "source": [
    "class YourModel(AudioModel):\n",
    "  def __init__(self, sr, n_fft, hop_length, n_mels, hidden_size, num_output):\n",
    "    super().__init__(sr, n_fft, hop_length, n_mels, hidden_size, num_output)\n",
    "    '''\n",
    "      TODO: Complete your new conv layer =\n",
    "      Example:\n",
    "      self.conv_layer = nn.Sequential(\n",
    "        nn.Conv1d(n_mels, out_channels=hidden_size, kernel_size=3),\n",
    "        nn.MaxPool1d(3),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv1d(hidden_size, out_channels=hidden_size, kernel_size=3),\n",
    "        nn.MaxPool1d(3),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "      '''\n",
    "    self.conv_layer = nn.Sequential(\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h4DRtuzjUj6e"
   },
   "outputs": [],
   "source": [
    "your_model = YourModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
    "optimizer = torch.optim.Adam(your_model.parameters(), lr=1e-3)\n",
    "your_model = your_model.to(DEV)\n",
    "your_train_record = train_model(your_model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n",
    "\n",
    "## Save the figure with comparison of default setting\n",
    "plt.figure(figsize=(8,16))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_record['loss'])\n",
    "plt.plot(your_train_record['loss'])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_record['valid_acc'])\n",
    "plt.plot(your_train_record['valid_acc'])\n",
    "save_fig_with_date('your_conv_layer_comparison_with_default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yzROUqOUj6e"
   },
   "source": [
    "### Probelm 4. Try Various Settings and Report (20 pts)\n",
    "- You can try different `n_fft`, `n_mels`, or `hidden_size`, or different `conv_layer` in your model\n",
    "- Describe the result and your analysis in your report\n",
    "    - Why you tried those changes\n",
    "    - What you have expected from the result with those changes\n",
    "    - What you got from the changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0h6XoqUUj6f"
   },
   "outputs": [],
   "source": [
    "your_model = YourModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=32)\n",
    "optimizer = torch.optim.Adam(your_model.parameters(), lr=1e-3)\n",
    "your_model = your_model.to(DEV)\n",
    "your_train_record = train_model(your_model, train_loader, valid_loader, optimizer, num_epochs=30, loss_func=loss_func, device=DEV)\n",
    "\n",
    "## Save the figure with comparison of default setting\n",
    "plt.figure(figsize=(8,16))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_record['loss'])\n",
    "plt.plot(your_train_record['loss'])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_record['valid_acc'])\n",
    "plt.plot(your_train_record['valid_acc'])\n",
    "save_fig_with_date('comparison_with_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get the test result\n",
    "'''\n",
    "test_acc = validate_model(your_model, test_loader, DEV)\n",
    "print(f\"Calculated ROC_AUC value for Test Set is : {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_PPNwM1Uj6f"
   },
   "source": [
    "### Problem 5 Complete Binary Cross Entropy Function (5 pts) \n",
    "- Complete the function that can calculate the Binary Cross Entropy for given prediction and target label without using `torch.BCELoss`\n",
    "- ![bce](https://androidkt.com/wp-content/uploads/2021/05/Selection_099.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWhH13CEUj6f"
   },
   "outputs": [],
   "source": [
    "def get_binary_cross_entropy(pred:torch.Tensor, target:torch.Tensor):\n",
    "  '''\n",
    "  pred (torch.Tensor): predicted value of a neural network model for a given input (assume that the value is output of sigmoid function)\n",
    "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
    "\n",
    "  output (torch.Tensor): Mean Binary Cross Entropy Loss value of every sample\n",
    "  '''\n",
    "  # TODO: Complete this function\n",
    "  return \n",
    "\n",
    "test_model = AudioModel(sr=16000, n_fft=1024, hop_length=512, n_mels=48, num_output=50, hidden_size=16)\n",
    "test_model = test_model.to(DEV)\n",
    "test_optimizer = torch.optim.Adam(test_model.parameters(), lr=1e-3)\n",
    "train_record = train_model(test_model, train_loader, valid_loader, test_optimizer, num_epochs=5, loss_func=get_binary_cross_entropy, device=DEV)\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(train_record['loss'])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(train_record['valid_acc'])\n",
    "save_fig_with_date('handmade_bce_result')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHpefphaUj6g"
   },
   "source": [
    "### Problem 6. Complete Precision-Recall Area Under Curve Function (20 pts)\n",
    "- One of the frequently used metric is Precision-Recall Area Under Curve (PR-AUC)\n",
    "- Precision is (Number of true positive)/(Number of total positive predictions)\n",
    "- Recall is (Number of true positive)/(Number of total positive ground-truth)\n",
    "- Precision and recall values depend on threshold\n",
    "- PR-AUC is the area under precision-recall curve of varying trheshold\n",
    "- You can refer the pre-defined `get_roc_auc` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SS9LLkWnUj6g"
   },
   "outputs": [],
   "source": [
    "def get_precision_and_recall(pred:torch.Tensor, target:torch.Tensor, threshold:float):\n",
    "  '''\n",
    "  This function calculates precision and recall of given (prediction, target, threshold)\n",
    "  \n",
    "  pred (torch.Tensor): predicted value of a neural network model for a given input \n",
    "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
    "\n",
    "  output\n",
    "    precision (torch.Tensor): (Number of true positive)/(Number of total positive predictions)\n",
    "    recall (torch.Tensor): (Number of true positive)/(Number of total positive ground-truth)\n",
    "    \n",
    "  IMPORTANT:\n",
    "    To prevent division by zero, make the denominator greater than zero.\n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  \n",
    "  # Write your code here\n",
    "  \n",
    "  precision = None\n",
    "  recall = None\n",
    "  \n",
    "  \n",
    "  '''\n",
    "  Be careful for not returning nan because of division by zero\n",
    "  '''\n",
    "  assert not (torch.isnan(precision) or torch.isnan(recall))\n",
    "  return precision, recall\n",
    "\n",
    "def get_precision_recall_auc(pred:torch.Tensor, target:torch.Tensor, num_grid=500):\n",
    "  '''\n",
    "  This function returns PR_AUC value for a given prediction and target.\n",
    "  Assume pred.shape == target.shape\n",
    "  \n",
    "  pred (torch.Tensor): predicted value of a neural network model for a given input \n",
    "  target (torch.Tensor): ground-truth label for a given input, given in multi-hot encoding\n",
    "\n",
    "  output (torch.Tensor): Area Under Curve value for Precision-Recall Curve\n",
    "  \n",
    "  TODO: Complete this function using get_precision_and_recall\n",
    "  '''\n",
    "\n",
    "  return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Test the get_precision_recall_auc\n",
    "'''\n",
    "\n",
    "dummy_pred = torch.Tensor([0.0285,     0.0004,     0.0483,     0.0003,     0.0074,     0.0141,\n",
    "            0.0007,     0.0735,     0.0534,     0.0153,     0.0024,     0.0053,\n",
    "            0.0004,     0.1033,     0.1007,     0.4314,     0.1744,     0.0119,\n",
    "            0.0189,     0.0075,     0.0001,     0.1354,     0.0014,     0.0004,\n",
    "            0.4431,     0.0236,     0.0005,     0.1276,     0.0173,     0.0000,\n",
    "            0.0010,     0.1237,     0.0616,     0.1674,     0.0000,     0.0053,\n",
    "            0.0984,     0.0608,     0.1783,     0.0689,     0.0509,     0.0011,\n",
    "            0.0749,     0.0001,     0.6105,     0.0136,     0.2644,     0.0204,\n",
    "            0.0005,     0.0001])\n",
    "dummy_target = torch.Tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "        0, 0])\n",
    "\n",
    "'''\n",
    "Printed result of code below has to be tensor(0.1753)\n",
    "'''\n",
    "get_precision_recall_auc(dummy_pred, dummy_target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = model\n",
    "pr_auc_value_valid = validate_model(selected_model, valid_loader, DEV, acc_func=get_precision_recall_auc)\n",
    "pr_auc_value_test = validate_model(selected_model, test_loader, DEV, acc_func=get_precision_recall_auc)\n",
    "print(f\"Calculated PR_AUC value for Validation Set is : {pr_auc_value_valid.item():.4f}\")\n",
    "print(f\"Calculated PR_AUC value for Test Set is : {pr_auc_value_test.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93nJtVppUj6g"
   },
   "source": [
    "### Problem 7: Load audio and make prediction (15 pts)\n",
    "- Upload mp3 file of your choice\n",
    "    - If you are using Colab, you can upload file by opening File Browser at the sidebar\n",
    "- Try several audio files and report the result by comparing your expectation and model's output\n",
    "- You can get different result by modifying `THRESHOLD`\n",
    "    - `THRESHOLD` has to be a value between 0 and 1\n",
    "    - If you lower the `THRESHOLD`, more tags will be printed out\n",
    "- Complete `slice_audio` function (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2fMCcJNGUj6g"
   },
   "outputs": [],
   "source": [
    "your_audio_path = 'your_audio_file_path' #TODO\n",
    "selected_model = model # Change it if you want to select model with different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWXlws--Uj6h"
   },
   "outputs": [],
   "source": [
    "def get_resampled_mono_audio_from_file(audio_file_path:str, target_sr:int):\n",
    "  y, sr = torchaudio.load(audio_file_path)\n",
    "  if sr!= target_sr:\n",
    "    y = torchaudio.functional.resample(y, orig_freq=sr, new_freq=target_sr)\n",
    "  if y.shape[0] > 1:\n",
    "    y = torch.sum(y, dim=0) / y.shape[0]\n",
    "  \n",
    "  return y\n",
    "\n",
    "def slice_audio(audio_sample:torch.Tensor, sr:int, start_sec:float, end_sec:float):\n",
    "  '''\n",
    "  This function takes an audio sample, sampling rate, and start/end position of slice\n",
    "  and returns the sliced audio sample.\n",
    "  \n",
    "  audio_sample (torch.Tensor): A sequence of audio samples in shape of (N,), where N is number of audio samples\n",
    "  sr (int): Sampling rate of audio_sample\n",
    "  start_sec (float): desired slice start position in seconds\n",
    "  end_sec (float): desired slice end position in seconds\n",
    "  \n",
    "  output (torch.Tensor): A sequence of audio samples in shape of (int(sr*(end_sec-start_sec)), ) \n",
    "  \n",
    "  \n",
    "  TODO: Complete this function\n",
    "  '''\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqzRixHZUj6h"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Run Model\n",
    "'''\n",
    "\n",
    "\n",
    "THRESHOLD = 0.2\n",
    "y = get_resampled_mono_audio_from_file(your_audio_path, selected_model.sr)\n",
    "\n",
    "'''\n",
    "You can slice your desired position\n",
    "'''\n",
    "sliced_y = slice_audio(y, selected_model.sr, 0, 30) \n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "  pred = selected_model(sliced_y.unsqueeze(0).to(DEV)).to('cpu')\n",
    "pred = pred[0]\n",
    "ipd.display(ipd.Audio(sliced_y, rate=selected_model.sr))\n",
    "print(f\"Predicted tags are: {model.vocab[torch.where(pred>THRESHOLD)]}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Creative Technologies Assignment1.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
